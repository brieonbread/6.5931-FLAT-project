{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fbc13b-fa37-40a1-9202-eb228c61cc3f",
   "metadata": {},
   "source": [
    "# LoopTree Tutorial\n",
    "\n",
    "LoopTree is a model to evaluate the latency and energy of a fused-layer dataflow accelerator.\n",
    "\n",
    "To model energy and latency, a workload, architecture, and mapping have to be specified. First, we discuss how these are specified. Then, we show how to run the LoopTree model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3ab36-56ff-471e-8c8a-9a70ac85ee97",
   "metadata": {},
   "source": [
    "## Specifying Architecture, Workload, and Mapping\n",
    "\n",
    "For the LoopTree model to estimate energy and latency, the user must specify an architecture, workload, and mapping. Below, we discuss how to specify each of these inputs.\n",
    "\n",
    "### Architecture\n",
    "In LoopTree, the architecture of an accelerator is abstracted as buffers that use the Buffet semantics and computation units, following the [Timeloop v4 specification](https://timeloop.csail.mit.edu/v4).\n",
    "\n",
    "An example architecture that we will use here is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6e2cf7-29eb-443d-885a-8c0262ca5edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables:\n",
      " global_cycle_seconds: 1e-9 # 1 / 0.7 GHz = 1.43 ns\n",
      " technology: \"45nm\"\n",
      "\n",
      "architecture:\n",
      " version: 0.4\n",
      " nodes:\n",
      " - !Component\n",
      "   name: MainMemory\n",
      "   class: DRAM            # off chip memory\n",
      "   Attributes:\t\t# do we have to set depth for off chip?\n",
      "       width: 3200\t# where do we find a value for this?\n",
      "       word_bits: 16\n",
      "       datawidth: 16\n",
      "       shared_bandwidth: 200 # words per cycle @ 10^9 cycles/s = 400 GB/s bandwidth\n",
      "   required_actions: ['read', 'write']\n",
      " - !Component\n",
      "   name: GlobalBuffer\n",
      "   class: SRAM           # on chip memory\n",
      "   attributes:  # does the depth/width/blksz matter as long as the capacity is right?\n",
      "       depth: 65536            # 4096 bits per row × 65536 rows = 32MB total\n",
      "       width: 4096             # Number of bits per row (256 words × 16 bits)\n",
      "       block_size: 256         # 256 words per row\n",
      "       word_bits: 16           # Each word is 16 bits (2 bytes)\n",
      "       datawidth: 16          \n",
      "       n_rdwr_ports: 4         # Where do we find a value for this?\n",
      "       shared_bandwidth: 4000  # words per cycle @ 1ns cycle time = 8 TB/s total bandwidth\n",
      "   required_actions: ['read', 'write']\n",
      " - !Container\n",
      "   name: PE\n",
      "   spatial:\n",
      "   meshX: 256\n",
      "    meshY: 256\n",
      " - !Component\n",
      "   name: PEWeightReg\n",
      "   class: regfile\n",
      "   attributes:\n",
      "\tdepth: 16\n",
      "\tWidth: 16\n",
      "\n",
      "\t\n",
      " - !Component\n",
      "   name: PECompute\n",
      "   class: fpmac\n",
      "   attributes:\n",
      "       datawidth: 16\n",
      "       width: 16\n",
      "       cycle_time: 1e-9\n",
      "   required_actions: ['compute']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from pprint import pp\n",
    "show_config('flat/architecture.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9392be-34e6-4d8e-9d96-9e5b3c0bc0c2",
   "metadata": {},
   "source": [
    "### Workloads\n",
    "\n",
    "DNN workloads in LoopTree are abstracted as a set of Einsums, each representing a layer.\n",
    "\n",
    "Each Einsum is specified as a dictionary. Each Einsum reads/writes to a number of tensors (referred to as `data_spaces` in the specification), and each tensor is either an input to the operation or an output (indicated by a key \"read_write\" with value `True`). Moreover, a \"projection\" from the Einsum to each tensor, which describes the index of the element in the data that an operation in the Einsum accesses, must be specified. Finally, we specify must specify the *shape* of the Einsum, which is the bounds of its dimensions (also referred to as \"ranks\").\n",
    "\n",
    "An example comprising two fully-connected layers are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab4d488-7b82-4a9d-8f15-1d3debe11aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem:\n",
      " - shape:\n",
      "     name: QK\n",
      "     dimensions: [ B1, H1, M1, P1, E1]\n",
      "     data_spaces:\n",
      "     - name: Q\n",
      "       dimensions: [ Q_B, Q_H, Q_M, Q_E ]\n",
      "       projection: '[ B1, H1, M1, E1 ]'\n",
      "     - name: K\n",
      "       dimensions: [ K_B, K_H, K_P, K_E ]\n",
      "       projection: '[ B1, H1, P1, E1 ]'\n",
      "     - name: L\n",
      "       dimensions: [ L_B, L_H, L_M, L_P ]\n",
      "       projection: '[ B1, H1, M1, P1 ]'\n",
      "       read_write: True\n",
      "\n",
      "   instance: >-\n",
      "    0 <= B1 < 64 and\n",
      "    0 <= H1 < 12 and\n",
      "    0 <= M1 < 512 and\n",
      "    0 <= P1 < 512 and\n",
      "    0 <= E1 < 64\n",
      "\n",
      "\n",
      " - shape:\n",
      "     name: SM\n",
      "     dimensions: [ B2, H2, M2, P2 ]\n",
      "     data_spaces:\n",
      "     - name: L\n",
      "       dimensions: [ L_B, L_H, L_M, L_P ]\n",
      "       projection: '[ B2, H2, M2, P2 ]'\n",
      "     - name: A\n",
      "       dimensions: [ A_B, A_H, A_M, A_P ]\n",
      "       projection: '[ B2, H2, M2, P2 ]'\n",
      "       read_write: True\n",
      "\n",
      "   instance: >-\n",
      "    0 <= B2 < 64 and\n",
      "    0 <= H2 < 12 and\n",
      "    0 <= M2 < 512 and\n",
      "    0 <= P2 < 512\n",
      "\n",
      "\n",
      " - shape:\n",
      "     name: AV\n",
      "     dimensions: [ B3, H3, M3, P3, F3 ]\n",
      "     data_spaces:\n",
      "     - name: A\n",
      "       dimensions: [ A_B, A_H, A_N, A_M ]\n",
      "       projection: '[ B3, H3, M3, P3 ]'\n",
      "     - name: V\n",
      "       dimensions: [ V_B, V_H, V_N, V_F ]\n",
      "       projection: '[ B3, H3, P3, F3 ]'\n",
      "     - name: O\n",
      "       dimensions: [ O_B, O_H, O_N, O_d ]\n",
      "       projection: '[ B3, H3, M3, F3 ]'\n",
      "       read_write: True\n",
      "\n",
      "   instance: >-\n",
      "    0 <= B3 < 64 and\n",
      "    0 <= H3 < 12 and\n",
      "    0 <= M3 < 512 and\n",
      "    0 <= P3 < 512 and\n",
      "    0 <= F3 < 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_config('flat/workload.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ea13d-6158-44a4-867c-530c52a1cd6a",
   "metadata": {},
   "source": [
    "In this workload specification, we specify two Einsums Fc1 and Fc2. Each Einsum has three dimensions. Fc1 has dimensions P1, M1, and C1; Fc2 has dimensions P2, M2, and C2. Finally, we specify the shape of the Einsums as bounds for each Einsum dimension (rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb0c8b9-59e2-4bb4-a6af-83f617b43c69",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "The LoopTree mapping is a tree-structure that contains nodes of the types described below.\n",
    "- **Loops**: a loop node specifies a rank in the Einsum that is partitioned and the shape of the tiles that results from the partitioning. If the loop is a \"temporal\" loop, then the tiles are scheduled to be processed one at a time. If the loop is a \"spatial\" loop, then the tiles are scheduled to parallel hardware units.\n",
    "- **Branching point**: nodes above a branching point (*i.e.*, ancestor nodes) describe inter-Einsum mapping, which is applied to all Einsums under that branching point. Nodes underneath the branching point (*i.e.*, within each branch) pertain only to the Einsum of that branch.\n",
    "- **Storage**: a storage node specifies the tiles of tensors to retain and which hardware level retains the tiles.\n",
    "- **Compute**: a compute node specifies the hardware level used to compute operations of an Einsum. This node has to be a leaf node, and it also denotes the Einsum that a particular branch pertains to.\n",
    "\n",
    "First, we discuss a layer-by-layer mapping example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd28a74-5330-4624-a414-e5ca545984ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: fused\n",
      "nodes:\n",
      "-type: storage\n",
      "target: 0 # MainMemory/DRAM\n",
      "dspace: [Q, K, V, O] # Exclude L and A, to reduce memory traffic\n",
      "\n",
      "\n",
      "-type: temporal # Tiling Q, K, V, L, A, O by size Bx, Hx, and Nx.\n",
      "rank: B1\n",
      "tile_shape: 256 # TODO - we need to define tile size\n",
      "-type: temporal \n",
      "rank: H1\n",
      "tile_shape: 256\n",
      "-type: temporal \n",
      "rank: M1\n",
      "tile_shape: 256 # - 256 # The goal of tiling\n",
      "\n",
      "-type: storage # store tiles of all tensors in SRAM\n",
      "target: 1 # GlobalBuffer/SRAM\n",
      "dspace: [Q, K, V, O, L, A]\n",
      "\n",
      "-type: sequential\n",
      "branches:\n",
      "\n",
      "# Now, branches for each layer (QK).\n",
      "# The amount of data sent to the PEs must be <= meshX * meshY. \n",
      "- - type: temporal\n",
      "    rank: B1\n",
      "    tile_shape: 1 \n",
      "- - type: temporal\n",
      "    rank: H1\n",
      "    tile_shape: 1 \n",
      "\n",
      "# Bringing the weight-stationary matrix into the correct tile to be distributed through the PEs\n",
      "- - type: temporal\n",
      "    rank: E1\n",
      "    tile_shape: 256\n",
      "- - type: temporal\n",
      "    rank: P1\n",
      "    tile_shape: 256\n",
      "\n",
      "# Qtile: Bx, Hx, Mx, Ex\n",
      "# Ktile: Bx, Hx, Px, Ex\n",
      "# Ltile: Bx, Hx, Mx, Px\t\n",
      "\n",
      "- type: spatial # for (0, 256, 1) spatial\n",
      "  rank: E1 # enables output reuse (prevents reloading space)\n",
      "  tile_shape: 1\n",
      "- type: spatial\n",
      "  rank: P1 # Enables input reuse, so we can apply to the weights along the column\n",
      "  tile_shape: 1 \n",
      "\n",
      "- type: storage\n",
      "  target: PEWeightStorage\n",
      "  dspace: [K]  # 1 * 1 * P * Ex\n",
      "\n",
      "- type: temporal # for (0, 256, 1) spatial\n",
      "  rank: M1 # enables output reuse (prevents reloading space)\n",
      "  tile_shape: 1\n",
      "\n",
      "- type: compute\n",
      "  einsum: QK\n",
      "  target: 2\n",
      "\n",
      "# ——————————————————————————————————————\n",
      "\n",
      "# Branch for softmax\n",
      "- - type: temporal\n",
      "    rank: B2\n",
      "    tile_shape: 1 \n",
      "- - type: temporal\n",
      "    rank: H2\n",
      "    tile_shape: 1 \n",
      "- - type: temporal\n",
      "    rank: M2\n",
      "    tile_shape: 256\n",
      "- - type: temporal\n",
      "    rank: P2\n",
      "    tile_shape: 256\n",
      "\n",
      "- -type: spatial\n",
      " rank: M2\n",
      "tile_shape: 1\n",
      "- -type: spatial\n",
      " rank: P2\n",
      "tile_shape: 1\n",
      "\n",
      "- type: storage\n",
      "  target: PEWeightStorage\n",
      "  dspace: [L]\n",
      "\n",
      "\n",
      "-type: compute\n",
      " einsum: SM\n",
      " target: 2\n",
      "\n",
      "# —————————————————————————————\n",
      "# Branch for AV\n",
      "# The amount of data sent to the PEs must be <= meshX * meshY. \n",
      "- - type: temporal\n",
      "    rank: B3\n",
      "    tile_shape: 1 \n",
      "- - type: temporal\n",
      "    rank: H3\n",
      "    tile_shape: 1 \n",
      "\n",
      "# Bringing the weight-stationary matrix into the correct tile to be distributed through the PEs\n",
      "- - type: temporal\n",
      "    rank: F3\n",
      "    tile_shape: 256\n",
      "- - type: temporal\n",
      "    rank: P3\n",
      "    tile_shape: 256\n",
      "\n",
      "- type: spatial\n",
      "  rank: F3 # enables output reuse (partial summing along a column of V^T)\n",
      "  tile_shape: 1\n",
      "- type: spatial\n",
      "  rank: P3 # Enables input reuse, so we can apply to the weights along the column\n",
      "  tile_shape: 1 \n",
      "\n",
      "- type: storage\n",
      "  target: PEWeightStorage\n",
      "  dspace: [V]\n",
      "\n",
      "- type: temporal \n",
      "  rank: M3\n",
      "  tile_shape: 1\n",
      "\n",
      "-type: compute\n",
      " einsum: AV\n",
      " target: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_config('flat/mapping.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146686f-b807-491b-af7c-82bfe83bcf53",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- (1.a) The storage node specifies that the tensors specified by the `dspace` key will be retained in target 0, which we will bind to MainMemory.\n",
    "        Note that MainMemory retains Fmap2.\n",
    "- (1.b) The sequential node specifies that the following branches (one for Fc1 and one for Fc2, as we will see shortly) are processed sequentially.\n",
    "- (1.c) Filter1 is retained in GlobalBuffer\n",
    "- (1.d) The rank P1 (which is a rank of the Fc1 Einsum) is partitioned into tiles with shape 1 and we will iterate over the tiles one at a time (temporal iteration).\n",
    "- (1.e) Tiles of Fmap1 and Fmap2 are retained in GlobalBuffer. We retain tiles becuase the P1 rank has been partitioned.\n",
    "- (1.f) Operations of Fc1 are processed in the MACC unit. Moreover, this node specifies that this branch is relevant to Fc2.\n",
    "- (1.g) This and the following nodes specify how Fc2 is mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3eaa59e-8b83-49ef-8f36-755135d6d5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:\n",
      "  type: fused\n",
      "  nodes:\n",
      "  - type: storage\n",
      "    target: 0  # level 2 is bound to MainMemory\n",
      "    dspace: [Filter1, Filter2, Fmap1, Fmap3] #-------- node 2.a\n",
      "  - type: storage\n",
      "    target: 1  # level 1 is bound to GlobalBuffer\n",
      "    dspace: [Filter1, Filter2]\n",
      "  - type: temporal #---------------------------------- node 2.b\n",
      "    rank: P2\n",
      "    tile_shape: 1\n",
      "  - type: storage  #---------------------------------- node 2.c\n",
      "    target: 1  # level 1 is bound to GlobalBuffer\n",
      "    dspace: [Fmap1, Fmap2, Fmap3]\n",
      "  - type: sequential  #------------------------------- node 2.d\n",
      "    branches:\n",
      "    - - type: temporal\n",
      "        rank: C1\n",
      "        tile_shape: 1\n",
      "      - type: temporal\n",
      "        rank: M1\n",
      "        tile_shape: 1\n",
      "      - type: compute\n",
      "        einsum: Fc1\n",
      "        target: 2  # level 0 is bound to MACC\n",
      "    - - type: temporal\n",
      "        rank: C2\n",
      "        tile_shape: 1\n",
      "      - type: temporal\n",
      "        rank: M2\n",
      "        tile_shape: 1\n",
      "      - type: compute\n",
      "        einsum: Fc2\n",
      "        target: 2  # level 0 is bound to MACC\n"
     ]
    }
   ],
   "source": [
    "show_config('fused.mapping.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213bff1-447f-4a80-922c-cef4c638d2eb",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- (2.a) Similar to (1.a), but note that Fmap2 is no longer retained in MainMemory.\n",
    "- (2.b) In this mapping, the $P2$ rank is partitioned to create tiles that are rows of feature maps Fmap1, Fmap2, and Fmap3. The tile shape attribute of the loop node implies that the tiles of Fmap3 have shape 1 in the $P2$ rank. LoopTree infers the shape of the tiles of Fmap2 and Fmap1 based on what is required as inputs to compute the specified tile of Fmap3. In this case, tiles that each contains one row of Fmap2 is required to compute tiles of Fmap3, and tiles that each contains one row of Fmap1 is required to compute tiles of Fmap2 in turn.\n",
    "- (2.c) The tiles of Fmap1, Fmap2, and Fmap3 are retained in GlobalBuffer.\n",
    "- (2.d) Similar to before, Fc1 and Fc2 are processed sequentially. However, as we have put node (2.b) above this sequential node, *tiles* of Fc1 and Fc2 are processed sequentially in alternating fashion: a tile of Fc1 produces a tile of Fmap2, the tile of Fmap2 is consumed by a tile of Fc2, another tile of Fc1 is produced, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc558a8-360c-42ab-a62b-80edd1745924",
   "metadata": {},
   "source": [
    "## Running the Model\n",
    "\n",
    "Using the PyTimeloop library, running the model is as simple as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292fab7-9306-4336-8c71-8ec462721f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87522bd4-a683-46ae-884f-b35f01a557e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "map::at",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# As previously mentioned, bindings map levels specified in the mapping\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# to the hardware units specified in the architecture spec.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m bindings \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMainMemory\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobalBuffer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACC\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m }\n\u001b[0;32m---> 14\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mrun_looptree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCONFIG_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ['flat/architecture.yaml', 'flat/workload.yaml', 'flat/mapping.yaml'],\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marchitecture.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflat/workload.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused.mapping.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ['flat/architecture.yaml', 'two_fc.workload.yaml', 'fused.mapping.yaml'],\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43mTMP_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcall_accelergy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytimeloop/looptree/run.py:32\u001b[0m, in \u001b[0;36mrun_looptree\u001b[0;34m(config_dir, paths, tmp_path, bindings, call_accelergy)\u001b[0m\n\u001b[1;32m     30\u001b[0m yaml_str \u001b[38;5;241m=\u001b[39m gather_yaml_configs(config_dir, paths)\n\u001b[1;32m     31\u001b[0m config \u001b[38;5;241m=\u001b[39m Config(yaml_str, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLooptreeModelApp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m workload \u001b[38;5;241m=\u001b[39m LooptreeWorkload\u001b[38;5;241m.\u001b[39mparse_cfg(config\u001b[38;5;241m.\u001b[39mroot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblem\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     36\u001b[0m spec \u001b[38;5;241m=\u001b[39m Specification\u001b[38;5;241m.\u001b[39mfrom_yaml_files([\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mstr\u001b[39m(config_dir \u001b[38;5;241m/\u001b[39m p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths\n\u001b[1;32m     38\u001b[0m ])\n",
      "\u001b[0;31mIndexError\u001b[0m: map::at"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from pprint import pp\n",
    "\n",
    "from pytimeloop.looptree.run import run_looptree\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'GlobalBuffer',\n",
    "    2: 'MACC'\n",
    "}\n",
    "\n",
    "stats = run_looptree(\n",
    "    CONFIG_DIR,\n",
    "    # ['flat/architecture.yaml', 'flat/workload.yaml', 'flat/mapping.yaml'],\n",
    "    ['architecture.yaml', 'flat/workload.yaml', 'fused.mapping.yaml'],\n",
    "    # ['flat/architecture.yaml', 'two_fc.workload.yaml', 'fused.mapping.yaml'],\n",
    "TMP_DIR,\n",
    "    bindings,\n",
    "    call_accelergy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf9e3f-e58a-4167-a1d7-499ec6fab091",
   "metadata": {},
   "source": [
    "The result returned by the model contains several information, such as latency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b6f2040-068f-4505-95aa-cf183f9632a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 1048576\n"
     ]
    }
   ],
   "source": [
    "print('Latency:', stats.latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95b0b1-e942-4362-ab76-818742ac2a78",
   "metadata": {},
   "source": [
    "and also energy, which is computed by calculating the number of actions to each hardware component and multiplying that with the energy per action estimated using the Accelergy tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4427d1fc-aedf-4ee6-8030-5809a2c92596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy:\n",
      "{('MainMemory', 'read'): 50331648.0,\n",
      " ('GlobalBuffer', 'read'): 380775448.57600003,\n",
      " ('GlobalBuffer', 'write'): 123521941.50400001,\n",
      " ('MainMemory', 'write'): 33554432.0,\n",
      " ('MACC', 'compute'): 886046.72}\n"
     ]
    }
   ],
   "source": [
    "print('Energy:')\n",
    "pp(stats.energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cbe63c-494f-48fc-b341-317ebb2123d0",
   "metadata": {},
   "source": [
    "Now, we compare these results with the fused mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e339a0d-e596-4145-aa2a-4a77faf7aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 1048576\n",
      "Energy:\n",
      "{('MainMemory', 'read'): 33554432.0,\n",
      " ('GlobalBuffer', 'read'): 380775448.57600003,\n",
      " ('GlobalBuffer', 'write'): 122579025.92,\n",
      " ('MainMemory', 'write'): 16777216.0,\n",
      " ('MACC', 'compute'): 886046.72}\n"
     ]
    }
   ],
   "source": [
    "from pytimeloop.looptree.run import run_looptree\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'GlobalBuffer',\n",
    "    2: 'MACC'\n",
    "}\n",
    "\n",
    "stats = run_looptree(\n",
    "    CONFIG_DIR,\n",
    "    ['architecture.yaml', 'two_fc.workload.yaml', 'fused.mapping.yaml'],\n",
    "    TMP_DIR,\n",
    "    bindings,\n",
    "    call_accelergy=True\n",
    ")\n",
    "print('Latency:', stats.latency)\n",
    "print('Energy:')\n",
    "pp(stats.energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e41bc-8a1e-4f57-a95f-c8bdd275731d",
   "metadata": {},
   "source": [
    "As we can see, the energy consumption due to MainMemory reads and writes have decreased significantly from fusion.\n",
    "\n",
    "Here, we have not modeled the impact of limited MainMemory bandwidth; thus, because the MACC unit utilization is the same (100%) in both cases, the latency is the same. However, if the layer-by-layer latency is limited by MainMemory bandwidth, fusion will also decrease latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "574293da-bca2-4383-820a-438468a50572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 528384\n",
      "Energy:\n",
      "{('MainMemory', 'read'): 33554432.0,\n",
      " ('GlobalBuffer', 'read'): 380775448.57600003,\n",
      " ('GlobalBuffer', 'write'): 122579025.92,\n",
      " ('MainMemory', 'write'): 16777216.0,\n",
      " ('MACC', 'compute'): 886046.72}\n"
     ]
    }
   ],
   "source": [
    "from pytimeloop.looptree.run import run_looptree\n",
    "\n",
    "# As previously mentioned, bindings map levels specified in the mapping\n",
    "# to the hardware units specified in the architecture spec.\n",
    "bindings = {\n",
    "    0: 'MainMemory',\n",
    "    1: 'GlobalBuffer',\n",
    "    2: 'MACC'\n",
    "}\n",
    "\n",
    "stats = run_looptree(\n",
    "    CONFIG_DIR,\n",
    "    ['architecture.yaml', 'two_fc.workload.yaml', 'fused-pipeline.mapping.yaml'],\n",
    "    TMP_DIR,\n",
    "    bindings,\n",
    "    call_accelergy=True\n",
    ")\n",
    "print('Latency:', stats.latency)\n",
    "print('Energy:')\n",
    "pp(stats.energy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
